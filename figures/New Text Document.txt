\section{Algorithms Taxi}

\begin{algorithm}
\caption{Temporal Difference - Taxi}\label{alg:euclid}
\begin{algorithmic}[1]
\begin{lstlisting}
   def get_action_probs(self, state, e=0.0):
        """ obtains the action probabilities 
        corresponding to epsilon-greedy policy
        - e: Threshold for exploitation (0) vs exploration(1).
        policy_s = np.ones(self.nA) * e / self.nA
        best_a = np.argmax(self.Q[state])
        policy_s[best_a] = 1 - e + (e / self.nA)
        return policy_s

    def step(self, state, next_action, next_reward, next_state, done):
        """ Update the agent's knowledge, 
        using the most recently sampled tuple.
        # SARSA
        prev_estimate = self.Q[state][next_action]
        if (done):
            next_estimate = 0
        else:
            #SARSA
            #next_estimate = self.Q[next_state][next_action]
            # EXPECTED SARSA
            policy_s = self.get_action_probs(state)
            next_estimate = np.dot(self.Q[next_state], policy_s)
            
        # TD Target
        tdTarget = next_reward + self.gamma * next_estimate
        # Set the action value function for the current state and next action.
        self.Q[state][next_action] 
        = prev_estimate + self.constantAlpha * (tdTarget - prev_estimate)

\end{lstlisting}
\end{algorithmic}
\end{algorithm}

The algorithm above and bellow begin by getting the probability of the actions by taking the set Epsilon value to retrieve the function  policy used for the Temporal difference learning strategy. The policy is entered into the step function and the action is updated. The values are put into a table similarly to Q-learning and the agent begins it's training.

\section{Algorithms Mountain Car}
\begin{algorithm}
\caption{Temporal Difference - Mountain Car}\label{alg:euclid}
\begin{algorithmic}[1]
\begin{lstlisting}
    def get_probs(self, Q_s, epsilon, nA):
        policy_s = np.ones(nA) * epsilon / nA
        best_a = np.argmax(Q_s)
        policy_s[best_a] = 1 - epsilon + (epsilon / nA)
        return policy_s

    def act(self, state, epsilon, reward=None, done=None, mode='train'):
        state = self.preprocess_state(state)
        nA = self.env.action_space.n
        if mode == 'test':
            # Test mode: Simply produce an action
            action = np.argmax(self.q_table[state])
        else:
            sarsa_action = np.random.choice(np.arange(nA), p=get_probs(self.q_table[state], epsilon, nA)) \
                                    if state in self.q_table else env.action_space.sample()
            self.q_table[self.last_state + (self.last_action,)] += self.alpha * \
                (reward + self.gamma * self.q_table[state][sarsa_action]- self.q_table[self.last_state + (self.last_action,)])

            do_exploration = np.random.uniform(0, 1) < self.epsilon
            if do_exploration:
                # Pick a random action
                action = np.random.randint(0, self.action_size)
            else:
                # Pick the best action from Q table
                action = np.argmax(self.q_table[state])
        self.last_state = state
        self.last_action = action
        return action
    

\end{lstlisting}
\end{algorithmic}
\end{algorithm}